{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GraphNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KoUj7ek3gWOA",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IsUagiCyx6Yq",
        "colab": {}
      },
      "source": [
        "#Import required libraries\n",
        "%autoreload 2\n",
        "!pip3 install --upgrade pip\n",
        "!pip3 install --upgrade torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install torchfusion\n",
        "!pip3 install tensorboardx\n",
        "!pip3 install pillow\n",
        "!pip3 install pydicom\n",
        "!pip3 install opencv-python\n",
        "!pip3 install -q keras==2.2.4\n",
        "!pip3 install --upgrade keras.utils\n",
        "!pip3 install nibabel\n",
        "!pip install graph_nets\n",
        "#!pip3 install --upgrade tensorflow-gpu\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import scipy\n",
        "import pydicom as dicom\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import nibabel as nib\n",
        "#import cv2\n",
        "import datetime\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow import nn, layers\n",
        "from tensorflow.contrib import layers as clayers \n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers.core import Dropout, Lambda, Dense, Flatten\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.merge import Add, Concatenate\n",
        "from keras.layers import Input, Activation, Flatten, ZeroPadding2D, UpSampling2D, MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.engine import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "from keras.utils import conv_utils\n",
        "\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import transforms, utils, datasets\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from IPython import display\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cO_QtyGW5Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Logger:\n",
        "\n",
        "    def __init__(self, model_name, data_name):\n",
        "        \n",
        "        self.model_name = model_name\n",
        "        self.data_name = data_name\n",
        "\n",
        "        self.comment = '{}_{}'.format(model_name, data_name)\n",
        "        self.data_subdir = '{}/{}'.format(model_name, data_name)\n",
        "\n",
        "        # TensorBoard\n",
        "        self.writer = SummaryWriter(comment=self.comment)\n",
        "\n",
        "    def log(self, error, epoch, n_batch, num_batches):\n",
        "\n",
        "        # var_class = torch.autograd.variable.Variable\n",
        "        if isinstance(error, torch.autograd.Variable):\n",
        "            error = error.data.cpu().numpy()\n",
        "\n",
        "        step = Logger._step(epoch, n_batch, num_batches)\n",
        "        self.writer.add_scalar(self.comment + \"/Error\", error, step)\n",
        "        \n",
        "\n",
        "    def display_status(self, epoch, num_epochs, n_batch, num_batches, error):\n",
        "        \n",
        "        if isinstance(error, torch.autograd.Variable):\n",
        "            error = error.data.cpu().numpy()\n",
        "        \n",
        "        clear_output()\n",
        "        print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
        "            epoch,num_epochs, n_batch, num_batches)\n",
        "             )\n",
        "        print('Loss: {:.4f}'.format(error))\n",
        "\n",
        "    def save_models(self, model, epoch):\n",
        "        out_dir = './data/models/{}'.format(self.data_subdir)\n",
        "        Logger._make_dir(out_dir)\n",
        "        torch.save(model.state_dict(),\n",
        "                   '{}/epoch_{}'.format(out_dir, epoch))\n",
        "\n",
        "    def close(self):\n",
        "        self.writer.close()\n",
        "\n",
        "    # Private Functionality\n",
        "\n",
        "    @staticmethod\n",
        "    def _step(epoch, n_batch, num_batches):\n",
        "        return epoch * num_batches + n_batch\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_dir(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxZTkKheW8pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UCLADataset(Dataset):\n",
        "          \n",
        "    def __init__(self, transforms=None):\n",
        "        \n",
        "        self.transforms = transforms\n",
        "        \n",
        "        #get data from private BitBucket (since the data is not available publicly)\n",
        "        os.chdir(\"/floyd/home/\")\n",
        "        user = getpass('BitBucket user')\n",
        "        password = getpass('BitBucket password')\n",
        "        os.environ['BITBUCKET_AUTH'] = user + ':' + password.replace(\"@\", \"%40\")\n",
        "\n",
        "        !git clone https://$BITBUCKET_AUTH@bitbucket.org/sidijju/ucla-data.git\n",
        "        \n",
        "        #Floydhub: \n",
        "        root = \"/floyd/home/ucla-data/connectivity-data\"\n",
        "\n",
        "        for dirname, subdirname, filenames in os.walk(root):\n",
        "            for filename in filenames:\n",
        "                #read connectivity matrix in\n",
        "                x = f.read().split(\" \")\n",
        "                arr = []\n",
        "                count = 0\n",
        "                for element in x:\n",
        "                    if element != '':\n",
        "                        arr.append(float(element))\n",
        "                        count += 1\n",
        "                size = int(math.sqrt(count))\n",
        "\n",
        "                img = [[0] * size for i in range(size)]\n",
        "                for i in range(size):\n",
        "                    for j in range(size):\n",
        "                        img[i][j] = arr[size*i + j]\n",
        "\n",
        "                if filename == filenames[0]:\n",
        "                    self.data = np.array(img)\n",
        "                else:\n",
        "                    self.data = np.concatenate((self.data, np.array(img)), axis = 2)\n",
        "            print(\"Data Shape\", self.data.shape)\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        if index < np.size(self.arr,0) and index >= 0:\n",
        "            if self.transforms is not None:\n",
        "                self.data.add(self.transforms(self.arr[index].astype('uint8')))\n",
        "            return self.data[index].astype('uint8'), index\n",
        "        else:\n",
        "            print(\"INDEX INVALID\")\n",
        "            return None\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMSCFUGHhqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ucla_data():\n",
        "    DATA_FOLDER = './tf_data/GNN/Data'\n",
        "    compose = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
        "    return UCLADataset(transforms = compose)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oit_bPiITKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "6c04c525-6ac6-410f-eb1e-61bed3615764"
      },
      "source": [
        "dataset = ucla_data()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-99b7cea88cd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mucla_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-89e009ec0b40>\u001b[0m in \u001b[0;36mucla_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         ])\n\u001b[1;32m      7\u001b[0m     \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}/dataset'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUCLADataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-acae209866c3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transforms)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#get data from private BitBucket (since the data is not available publicly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/floyd/home/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BitBucket user'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BitBucket password'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/floyd/home/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhzB7DVMGjoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "g = generator_model()\n",
        "#plot_model(g, to_file='generator_name.png', show_shapes=True, show_layer_names=True)\n",
        "g.load_weights(\"generator_sr50_\" + str(currEpoch) + \".h5\")\n",
        "print(\"GENERATOR\")\n",
        "g.summary()\n",
        "\n",
        "d = discriminator_model()\n",
        "#plot_model(d, to_file='discriminator_name.png', show_shapes=True, show_layer_names=True)\n",
        "d.load_weights(\"discriminator_sr50_\" + str(currEpoch) + \".h5\")\n",
        "print(\"DISCRIMINATOR\")\n",
        "d.summary()\n",
        "\n",
        "d_on_g = gendis_mult_out(g, d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpjR3CBEdyBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_all_weights(d, g, epoch_number, current_loss):\n",
        "    now = datetime.datetime.now()\n",
        "    os.chdir(\"/floyd/home/\")\n",
        "    save_dir = os.path.join(\"weights/\", '{}_{}_{} run'.format(now.month, now.day, now.year))\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    g.save_weights(os.path.join(save_dir, 'generator_sr{}_{}.h5'.format(SAMPLING_RATE, epoch_number + currEpoch)), True)\n",
        "    d.save_weights(os.path.join(save_dir, 'discriminator_sr{}_{}.h5'.format(SAMPLING_RATE, epoch_number + currEpoch)), True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hkWl-TnMFM5F",
        "colab": {}
      },
      "source": [
        "# Initialize optimizers\n",
        "g_opt = Adam(epsilon=1E-4)\n",
        "#lr - sr2 - 1E-4\n",
        "d_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "d_on_g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "# Compile models\n",
        "d.trainable = True\n",
        "d.compile(optimizer=d_opt, loss=d_loss)\n",
        "d.trainable = False\n",
        "\n",
        "loss = [g_loss, d_loss]\n",
        "loss_weights = [100, 1]\n",
        "d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
        "d.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLVa88GbdyBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_true_batch, output_false_batch = np.random.uniform(low = 0.9, high = 1.0, size = (BATCH_SIZE, 1)), np.zeros((BATCH_SIZE, 1))\n",
        "\n",
        "logger = Logger(model_name='MRIGAN Trial 14.5 SR ' + str(SAMPLING_RATE) + '%', data_name='IXI')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y0AXkm2PFM5J",
        "colab": {}
      },
      "source": [
        "x_train, y_train = get_data_simple_IXI(dataset, sampling_rate = int(100/SAMPLING_RATE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Js6cKZFFM5Q",
        "colab": {}
      },
      "source": [
        "for epoch in tqdm.tqdm(range(EPOCH_NUM)):\n",
        "    \n",
        "    num_samples = int(x_train.shape[2])\n",
        "    \n",
        "    permutated_indexes = np.random.permutation(num_samples)\n",
        "    num_batches = int(num_samples / BATCH_SIZE)\n",
        "    \n",
        "    d_losses = []\n",
        "    d_on_g_losses = []\n",
        "    \n",
        "    for n_batch in range(num_batches):\n",
        "        batch_indexes = permutated_indexes[n_batch*BATCH_SIZE:(n_batch+1)*BATCH_SIZE]\n",
        "\n",
        "        sampled_batch = K.eval(K.expand_dims(np.rollaxis(x_train[:, :, batch_indexes], 2, 0), axis = -1))\n",
        "        full_batch = K.eval(K.expand_dims(np.rollaxis(y_train[:, :, batch_indexes], 2, 0), axis = -1))\n",
        "\n",
        "        generated_images = g.predict(x=sampled_batch, batch_size=BATCH_SIZE)\n",
        "\n",
        "        d_loss_real = d.train_on_batch(full_batch, output_true_batch)\n",
        "        d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
        "        d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "        d_losses.append(d_loss)\n",
        "\n",
        "        d.trainable = False\n",
        "\n",
        "        d_on_g_loss = d_on_g.train_on_batch(sampled_batch, [full_batch, output_true_batch])\n",
        "        d_on_g_losses.append(d_on_g_loss)\n",
        "\n",
        "        d.trainable = True\n",
        "        \n",
        "        logger.log(d_loss, d_on_g_loss, epoch, n_batch, num_batches) \n",
        "        logger.log_images(generated_images, full_batch, sampled_batch, BATCH_SIZE, epoch, n_batch, num_batches, format='NHWC')   \n",
        "    \n",
        "    #logger.display_status(epoch, EPOCH_NUM, n_batch, num_batches, d_loss, d_on_g_loss, d_loss_real, d_loss_fake)\n",
        "        \n",
        "       \n",
        "    #print(np.mean(d_losses), np.mean(d_on_g_losses))\n",
        "    with open('log.txt', 'a+') as f:\n",
        "        f.write('{} - {} - {}\\n'.format(epoch, np.mean(d_losses), np.mean(d_on_g_losses)))\n",
        "\n",
        "    save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}